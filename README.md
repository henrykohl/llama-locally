# llama-locally
How to Run LLaMA Locally on CPU or GPU | Python &amp; Langchain &amp; CTransformers Guide

下載一個 Model
> `wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_0.bin --quiet`
