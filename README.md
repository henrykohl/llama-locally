# llama-locally
How to Run LLaMA Locally on CPU or GPU | Python &amp; Langchain &amp; CTransformers Guide
